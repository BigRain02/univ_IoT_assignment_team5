# Eye tracking AI

<table>
    <tr>
        <td> This project is part of the mobile systems engineering class _Embedded systems and IoT_ at Dankook university(단국대학교). We are team 5 and are working on an algorithm/AI-based program to detect the direction a person is looking to, also known as eye tracking. Our motivation is to help people improve their presentation skills by collecting data about where they look, i.e. the audience, the screen or their notes, but we also see many more uses like helping drivers stay focused on the street or companies testing their advertisement and whether it "catches" the public's eye. </td>
    </tr>
</table>

<br/>
<br/>
<br/>
## Overview

We use a 2-staged structure to simplify each module and also keep them modular, so that retraining one model should not impact the other model. In this particular case, we are using the first stage to find any eyes in the current frame, and the second stage takes the eye-cutout, tries finds the pupil, and and with that information figure out where the person is looking.
There is a lot of room to improve detection without changing the models, since knowing that humans have two eyes, sanity checks through cross-checking can be performed on the output. 

![Concept poster of the steps involved in detecting the person's eyes and where they look](/team5-poster.png)

## Stage 1
Stage 1 uses [YOLOv8 by ultralytics](https://github.com/ultralytics/ultralytics) to find any eyes in the frame. The dataset used is a sub-selection of the dataset Labeled Faces in the Wild, short [LFW](https://www.kaggle.com/datasets/atulanandjha/lfwpeople), trained in [colab.research.google.com](https://colab.research.google.com). The model is trained on 532 images and validated with another 354. The exact subset used, split by training and validation, can be found [here](/customed_dataset/images). 

When the trained model is run with webcamTest.py, images are retrieved from the connected camera (by default the one with id=0), each frame is then checked for the presence of eyes. The frame together with the detection box are then displayed in an application window.

The following image is from the window generated by webcamTest.py using a virtual camera input from obs studio. The image is part of the training dataset.

![Screenshot of a person whose eyes are being detected using this AI model](/김대중%20eye%20detection.PNG)

The same way the box is drawn, these boxes can be saved as images to be used by stage-2.

## Stage 2

In stage 2, we want to develop a more complete detection system that goes beyond eye detection to include pupil detection, and develop automated eye detection and feedback algorithms accordingly.

The training dataset for this task is labeled as follows.  

![labelled image for training (tags hidden)](https://github.com/lunash0/IoT_team5/assets/109780232/09607791-9206-428b-9543-9311e58f0a6f)

We also worked on additional procedures to make detection more reliable, such as

- Adjusting the settings to only use high-confidence detections and only one eye, to avoid duplicates
- Preventing the right or left eye from being recognized twice instead of recognizing a pair of eyes.

The result of the eye and pupil detection:

![demonstration of successful detection of both eyes and pupils](https://github.com/lunash0/IoT_team5/assets/109780232/9ac3dd58-9f1d-4286-9e87-2d93430163a8)

